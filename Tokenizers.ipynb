{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d15fec-f16d-47b4-923d-24e3243b479c",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f55e0de-939a-4e41-b401-847b39136ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iammu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b757c4-fc46-4056-9802-973f2d07d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'know', 'you', 'like', 'my', 'articles', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I know you like my articles.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efb188-c857-4ceb-8383-eb2131f5d624",
   "metadata": {},
   "source": [
    "### Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3caac62f-ea58-4e2a-94c8-d07e94a24d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial Intelligence revolutionizes industries, unlocking new possibilities.', 'Through machine learning, AI systems adapt and improve over time.', \"AI's potential to automate tasks reshapes the future of work.\", 'Ethical considerations guide the responsible development and deployment of AI.', 'In healthcare, AI enhances diagnosis and treatment, saving lives.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iammu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download the punkt tokenizer if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Text to be tokenized into sentences\n",
    "text = \"Artificial Intelligence revolutionizes industries, unlocking new possibilities. Through machine learning, AI systems adapt and improve over time. AI's potential to automate tasks reshapes the future of work. Ethical considerations guide the responsible development and deployment of AI. In healthcare, AI enhances diagnosis and treatment, saving lives.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Print the tokenized sentences\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf4468-dab7-40f0-8cdd-830c4502bedd",
   "metadata": {},
   "source": [
    "### Character Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce615a4a-78de-4032-98bc-6a5693169712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!\"\n",
    "characters = list(text)\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54275286-b624-4d6a-bbc2-44a409ccae98",
   "metadata": {},
   "source": [
    "### Subword Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da435da2-309b-426a-a057-ce86c8dd793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=[\"./data/sample_corpus.txt\"], vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# Save the trained tokenizer\n",
    "tokenizer.save(\"./data/trained_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd78117-9351-4cdc-b7ac-f52aa0be5dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST_ENV",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
